[{"content":"In today\u0026rsquo;s fast-paced software development environment, the efficient and reliable deployment of applications is crucial. A Continuous Integration/Continuous Deployment (CI/CD) pipeline plays a pivotal role in automating and streamlining the software development lifecycle. This pipeline integrates Jenkins, Ansible, and cloud services from AWS and Azure to deliver a comprehensive solution for your software deployment needs.\nIn this article, we will demonstrate the following :\nThe Web Application will be deployed using python Flask framework.\nThe Web Application should be accessible via web browser from anywhere on port 4000.\nEC2\u0026rsquo;s and their security groups should be created on AWS console with Terraform.\nThe rest of the process has to be controlled with control node which is connected SSH port.\nPostgreSQL and Flask App should be running in containers\nWe are going to use AWS ECR as an image repository, to create Docker Containers with 2 managed nodes (postgresql and Flask instances).\nProject GitHub Repo: https://github.com/Malek-Zaag/Automation-pipeline-with-Ansible-and-AWS\nProject Requirements and architecture : Virtual machine for all the workload (Azure VM)\n2 Virtual machines for application (Flask) and postgresql as docker images\nAnsible\nAWS CLI to fetch images from AWS ECR\nCoding The application: Our application is a simple python Flask application that has a default route, add_book route and a delete route.\n@app.route(\u0026#39;/\u0026#39;) def index(): books = Book.query.all() return render_template(\u0026#39;index.html\u0026#39;, books=books) @app.route(\u0026#39;/add_book\u0026#39;, methods=[\u0026#39;POST\u0026#39;,\u0026#39;GET\u0026#39;]) def add_book(): if request.method == \u0026#39;POST\u0026#39;: title = request.form[\u0026#39;title\u0026#39;] author = request.form[\u0026#39;author\u0026#39;] publication_date = request.form[\u0026#39;publication_date\u0026#39;] book = Book(title=title, author=author, publication_date=publication_date) db.session.add(book) db.session.commit() return redirect(url_for(\u0026#39;index\u0026#39;)) elif request.method ==\u0026#39;GET\u0026#39;: return render_template(\u0026#39;form.html\u0026#39;) @app.route(\u0026#39;/delete_book/\u0026lt;int:id\u0026gt;\u0026#39;) def delete_book(id): book = Book.query.get(id) db.session.delete(book) db.session.commit() return redirect(url_for(\u0026#39;index\u0026#39;))py Creating custom Postgres docker image: As we want to have a custom user and password for our database, we opted for a building a new docker image for it and it creates a table called books and has persistent volumes for our data.\nFROM postgres:12.16-bullseye\rENV POSTGRES_USER=admin\rENV POSTGRES_PASSWORD=admin\rENV POSTGRES_HOST=localhost\rVOLUME [ \u0026quot;/var/lib/postgresql/data\u0026quot; ]\rWORKDIR /\rCOPY init.sql /docker-entrypoint-initdb.d/\rEXPOSE 5432do\rInit.sql:\nCREATE TABLE book ( id SERIAL PRIMARY KEY, title VARCHAR(255), author VARCHAR(255), publication_date DATE );\rAfter starting our containers, the application is working fine on our local machine :\nWriting Infrastructure manifest : As we mentioned earlier, we will be provisioning a jenkins server and it will be also operating as ansible admin server.\nFinished writing infrastructure manifests :\nNow it is time to provision the jenkins main server which will execute our pipeline, for that i used also the IAC approach :\nterraform apply - auto-approve - var-file=terraform.tfvars\rNow it is time to setup the jenkins server and install the appropriate plugins :\nWe need to setup our AWS access tokens in the machine so it can apply the manifest files:\nConfiguring aws ECR : Amazon Elastic Container Registry (Amazon ECR) is a fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere.\nWe create the private repository :\nPushign images to ECR : To push our local images to the repository that we have previously created, we need to tag the image like the following :\nDocker tag local_image_name repo_name:local_image\rDatabase :\nNow for ansible to connect to machine, we need to copy the private key to the jenkins server :\nOne more thing we needed to do is copying manually the vars.yaml file from our machine to the jenkins server because it contains vulnerable credentials and it cannot be pushed to git:\nAfter a lot of troubleshooting and tweaking of the ansible playbook, our pipeline executed successfully :\nOur playbook consists of installing required packages, installing docker and aws cli, and finally connecting docker to our private repository :\nWe can now access our Flask application located on the first machine, it seems working perfectly :\nBefore closing up, i want to explain a stage in our pipeline :\nstage(\u0026quot;Adding public IPs to /etc/hosts\u0026quot;) {\rsteps {\rdir(\u0026quot;./Infrastructure/EC2/\u0026quot;) {\rsh \u0026quot;terraform output \u0026gt; file.txt\u0026quot;\rsh \u0026quot;cut -d '=' -f 2 file.txt\u0026quot;\rsh \u0026quot;sed 's/\\\u0026quot;//g; s/=//g' file.txt \u0026gt; res.txt\u0026quot;\rsh 'awk \\' { t = $1; $1 = $2; $2 = t; print; } \\' res.txt \u0026gt; f.txt'\rsh \u0026quot;cat f.txt\u0026quot;\rsh 'sudo -- sh -c \u0026quot;cat f.txt \u0026gt;\u0026gt; /etc/hosts\u0026quot;'\r}\r}\r}\rThis little script takes the two ip addresses of the EC2s from the terraform output and it concatenates them into /etc/hosts file as :\nip_address hostname\rSo ansible can resolve ip-one and ip-two to their correct addresses.\nWas this helpful?\nConfusing?\nIf you have any questions, feel free to comment below!\nBefore you leave:\nüëè Clap for the story\nüì∞ Subscribe for more posts like this @malek.zaag ‚ö°Ô∏è\nüëâüëà Please follow me: GitHub | LinkedIn\n","permalink":"https://malek-zaag.netlify.app/blog/aws-automated-pipeline/","summary":"In today\u0026rsquo;s fast-paced software development environment, the efficient and reliable deployment of applications is crucial. A Continuous Integration/Continuous Deployment (CI/CD) pipeline plays a pivotal role in automating and streamlining the software development lifecycle. This pipeline integrates Jenkins, Ansible, and cloud services from AWS and Azure to deliver a comprehensive solution for your software deployment needs.\nIn this article, we will demonstrate the following :\nThe Web Application will be deployed using python Flask framework.","title":"Automating Deployment using Jenkins,Ansible and AWS"},{"content":"1. AWS Cloud Practitionner Date Earned: June 9, 2023 ","permalink":"https://malek-zaag.netlify.app/certifications/aws/","summary":"1. AWS Cloud Practitionner Date Earned: June 9, 2023 ","title":"AWS Certifications"},{"content":"CI/CD pipeline with Jenkins and Argo CD Overview : Hello, in this article we are going to build a CI/CD pipeline from scratch and we are going to talk about each component in it .\nHere is our pipeline :\nJenkins : What is Jenkins and what it is used for ?\nJenkins is an open source continuous integration/continuous delivery and deployment (CI/CD) automation software DevOps tool written in the Java programming language. It is used to implement CI/CD workflows, called pipelines.\nPipelines : Let ‚Äòs talk about what is a CI/CD pipeline .\nA continuous integration and continuous deployment (CI/CD) pipeline is a series of steps that must be performed in order to deliver a new version of software. CI/CD pipelines are a practice focused on improving software delivery throughout the software development life cycle via automation.\nInstalling Jenkins on Windows :\nTo install Jenkins, we go straight ahead to its official website and download the .msi file and we install it :\nNow it is time to set up credentials .\nCredentials :\nTo let Jenkins access my repository I need to configure the credentials for my github account :\nAlso, i must configure the azure credentials to my azure container registry so it can push the image there :\nMy Jenkinsfile :\npipeline{\nagent any\nenvironment{\nAZURE_REPO=\u0026lsquo;myprivaterepo.azurecr.io\u0026rsquo;\n}\nstages{\nstage(\u0026ldquo;build image\u0026rdquo;){\nsteps{\nsh \u0026ldquo;docker build -t myprivaterepo.azurecr.io/library-management-api ./api/\u0026rdquo;\necho \u0026ldquo;image built successfully\u0026rdquo;\n}\n}\nstage(\u0026ldquo;push image to ACR\u0026rdquo;){\nsteps{\necho \u0026ldquo;pushing to ACR stage\u0026rdquo;\nwithCredentials([usernamePassword(credentialsId: \u0026lsquo;azure-credentials\u0026rsquo;, passwordVariable: \u0026lsquo;PASS\u0026rsquo;, usernameVariable: \u0026lsquo;USER\u0026rsquo;)]){\nsh \u0026ldquo;echo $PASS | docker login ${AZURE_REPO} \u0026ndash;username $USER \u0026ndash;password-stdin\u0026rdquo;\nsh \u0026ldquo;nslookup myprivaterepo.azurecr.io\u0026rdquo;\nsh \u0026ldquo;docker push myprivaterepo.azurecr.io/library-management-api\u0026rdquo;\n}\necho \u0026ldquo;image pushed successfully to azure container registry\u0026rdquo;\n}\n}\n}\n}\nThen i got this error :\nThis error was solved by adding sh.exe to my PATH .\nNow our pipeline is running successfully, it is triggered by branch commits, rebuilds docker image every time and pushes it to Azure Container Registry :\nAzure MySQL Database : All of our application data is held in Azure Database for MySQL Databases, Azure gives us this service for free under certain Conditions :\nWe create flexible server :\nCreate a new database :\nAnd it is all set up . One more thing to talk about is the continuous Deployment .\nArgo CD : Argo CD\nArgo CD is a Kubernetes-native continuous deployment (CD) tool. Unlike external CD tools that only enable push-based deployments, Argo CD can pull updated code from Git repositories and deploy it directly to Kubernetes resources. It enables developers to manage both infrastructure configuration and application updates in one system.\nGitOps with Argo CD : GitOps is a software engineering practice that uses a Git repository as its single source of truth. Teams commit declarative configurations into Git, and these configurations are used to create environments needed for the continuous delivery process. There is no manual setup of environments and no use of standalone scripts ‚Äî everything is defined through the Git repository.\nA basic part of the GitOps process is a pull request. New versions of a configuration are introduced via pull request, merged with the main branch in the Git repository, and then the new version is automatically deployed. The Git repository contains a full record of all changes, including all details of the environment at every stage of the process.\nWe set up a new project and we link it to our git repository and specify where the k8s manifests are located :\nNow we can see our resources deployed successfully to Azure Kubernetes Cluster :\nJust a reminder, ArgoCD is installed on the cluster itself and is running as a LoadBalancer type service\nTesting : Testing our external api :\nWe test if the books are set :\nAnd finally we check the users :\nSurely, we can‚Äôt find them because we haven‚Äôt added yet .\nTo conclude, we learnt in this article about pipelines and GitOps, the way to deploy one and the tools needed for that .\n","permalink":"https://malek-zaag.netlify.app/blog/ci-cd-argocd/","summary":"CI/CD pipeline with Jenkins and Argo CD Overview : Hello, in this article we are going to build a CI/CD pipeline from scratch and we are going to talk about each component in it .\nHere is our pipeline :\nJenkins : What is Jenkins and what it is used for ?\nJenkins is an open source continuous integration/continuous delivery and deployment (CI/CD) automation software DevOps tool written in the Java programming language.","title":"CI/CD pipeline with Jenkins and Argo CD"},{"content":"In this article, we are going to demonstrate how to deploy a web application in AKS (Azure Kubernetes Service) .\nAzure Kubernetes Service\nAzure Kubernetes Service is a managed container orchestration service based on the open source Kubernetes system, which is available on the Microsoft Azure public cloud. An organization can use AKS to handle critical functionality such as deploying, scaling and managing Docker containers and container-based applications.\nAzure Container Registry\nAzure Container Registry\nAzure Container Registry is a private registry service for building, storing, and managing container images and related artifacts. In this quickstart, you create an Azure container registry instance with the Azure portal.\nNow we are going to push our images into the container registry:\ndocker push myprivaterepo.azurecr.io/client\ndocker push myprivaterepo.azurecr.io/server\nNow we can see our images pushed successfully to the remote registry :\nCreate our Kubernetes cluster\nWe are going now to create our cluster, we connect to the azure portal then we search for kubernetes service .\nFor the main page, we set the resource group and the cluster name. For the node agent, we set it to Standard_B2s which is the cheapest one available :\nFor networking inside the cluster, we used the default Kubenete network configuration and Calico for connectivity between pods and services :\nFinally, we connect our docker registry to the cluster so we can pull images :\nand we hit Create .\nDeploy the app\nNow, we need to connect to the cluster\naz account set \u0026ndash;subscription ####################\naz aks get-credentials \u0026ndash;resource-group aks-rg1 \u0026ndash;name mk-aks\nWe deploy our deployments and services using :\nkubectl apply -f .\\client-deploy.yaml\nkubectl apply -f .\\server-deploy.yaml\nkubectl apply -f .\\client-svc.yaml\nkubectl apply -f .\\server-svc.yaml\nNote : We are going to use the public cloud LoadBalancer to achieve that our front is served in the internet .\nNginx reverse-proxy : nginx\nTo achieve forwarding the API requests to our ClusterIP backend, we are going to use a nginx server which we will serve as a reverse-proxy .\na reverse proxy is the application that sits in front of back-end applications and forwards client (e.g. browser) requests to those applications. Reverse proxies help increase scalability, performance, resilience and security. The resources returned to the client appear as if they originated from the web server itself.\nMy nginx.conf :\nserver {\nlisten 80;\nlocation /api {\rproxy_http_version 1.1;\rproxy_set_header Upgrade $http_upgrade;\rproxy_set_header Connection \u0026quot;upgrade\u0026quot;;\rproxy_set_header Host $host;\rproxy_cache_bypass $http_upgrade;\rproxy_pass http://${API}:${PORT}/api;\r}\rlocation / {\rroot /usr/share/nginx/html;\rindex index.html index.htm;\rtry_files $uri $uri/ /index.html;\r}\rerror_page 500 502 503 504 /50x.html;\rlocation = 50x.html {\rroot /usr/share/nginx/html;\r}\r}\nIn this configuration, every request for ‚Äú/api‚Äù we will be forwarded to ‚Äúhttp://${API}:${PORT}/api‚Äù. For example, a request a /api/user will be forwarded to http://${API}:${PORT}/api/user .\nTesting out things : So, our services are now available and we can see our the public IP of our loadbalancer that serves our frontend application :\nwe access it and our application is running successfully :\nWe try to create and account and login :\nNow we create a new user to test our API :\nand our API works successfully :\nI hope you enjoyed reading the article and you learnt how to deploy a fullstack web application using the Azure Kubernetes Service .\n","permalink":"https://malek-zaag.netlify.app/blog/deploy-app-aks/","summary":"In this article, we are going to demonstrate how to deploy a web application in AKS (Azure Kubernetes Service) .\nAzure Kubernetes Service\nAzure Kubernetes Service is a managed container orchestration service based on the open source Kubernetes system, which is available on the Microsoft Azure public cloud. An organization can use AKS to handle critical functionality such as deploying, scaling and managing Docker containers and container-based applications.\nAzure Container Registry","title":"I Deployed application on Azure Kubernetes Service"},{"content":"Kubernetes setup locally using kubeadm Hello reader, today i will show you in this article how i managed to install a Kubernetes cluster on my computer using kubeadm, this cluster contained two worker nodes with 2 GB of ram each alongside with one master node (2.5 GB of ram) so it was pretty heavy on my computer to be honest.\nBut first of all, let‚Äôs talk about kubernetes.\nKubernetes So, kubernetes(we will call it k8s) is a container orchestration tool which allows you to run multiple containerized services and it let‚Äôs you to scale your application with ease.\nWhat do we find inside a Kubernetes cluster? We can represent a kubernetes cluster by the following diagram So, we are going to go through every component and explain its role:\nMaster Node: The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. Kube-apiserver: The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. Etcd: Consistent and highly available key value store used as Kubernetes‚Äô backing store for all cluster data. Kube-scheduler: Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. Kube-controller-manager: Control plane component that runs controller processes.Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. Worker Node: Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods. Kubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. Kube-proxy: kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. Container runtime: The container runtime is the software that is responsible for running containers. Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface). Kubernetes setup using kubeadm Kubeadm kubeadm helps you start a minimal, viable, and best-practice Kubernetes cluster. With kubeadm, your cluster must pass Kubernetes Conformance testing . Kubeadm also supports other lifecycle functions, such as upgrades, downgrades, and bootstrap token management . I managed to create these 3 virtual machines Ubuntu 18.04 on VMware with 2 vCPU each and i connected them via Bridged network.\nBridged network Bridging is distinct from routing. Routing allows multiple networks to communicate independently and yet remain separate, whereas bridging connects two separate networks as if they were a single network.\nSo after creating the vms and connecting them , i installed openssh-server on every machine so the sshd could be enabled and i sshed to each one of them . Now we can start working .\nPrepare the environments The following Steps must be applied to each node (both master nodes and worker nodes).\nDisable the Swap memory The Kubernetes requires that you disable the swap memory in the host system because the kubernetes scheduler determines the best available node on which to deploy newly created pods. If memory swapping is allowed to occur on a host system, this can lead to performance and stability issues within Kubernetes .\nYou can disable the swap memory by deleting or commenting the swap entry in /etc/fstab manually or by running the following command\nsudo swapoff -a Configure or Disable the firewall When running Kubernetes in an environment with strict network boundaries, such as on-premises datacenter with physical network firewalls or Virtual Networks in Public Cloud, it is useful to be aware of the ports and protocols used by Kubernetes components\nThe ports used by Master Node : The ports used by Worker Nodes : You can either disable the firewall (not recommended) or allow the ports on each node.\nAdd firewall rules to allow the ports used by the kubernetes nodes Allow the ports used by the master node :\nmk@k8s-master:~$ sudo ufw allow 6443/tcp\r[sudo] password for mk:\rRules updated\rRules updated (v6) and we follow up with other ports .\nAllow the ports used by the worker nodes :\nmk@k8s-worker1:~$ sudo ufw allow 10250/tcp\r[sudo] password for mk:\rRules updated\rRules updated (v6) and we follow up with other ports .\nInstalling Docker Engine Kubernetes requires you to install a container runtime to work correctly.There are many available options like containerd, CRI-O, Docker etc\nBy default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.If you don‚Äôt specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.\nYou must install the Docker Engine on each node!\nSet up the repository\nsudo apt update\rsudo apt install ca-certificates curl gnupg lsb-release Add Docker‚Äôs official GPG key\nsudo mkdir -p /etc/apt/keyrings\rcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg - dearmor -o /etc/apt/keyrings/docker.gpg Add the stable repository using the following command\necho \\\r\u0026gt; \u0026#34;deb \\[arch=$(dpkg - print-architecture) signed-by=/etc/apt/keyrings/docker.gpg\\] https://download.docker.com/linux/ubuntu \\\r\u0026gt; $(lsb\\_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null Install the docker engine\nsudo apt-get update\rsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Make sure that the docker will work on system startup\nsudo systemctl enable - now docker Configuring the Cgroup Driver\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/docker/daemon.json\r{\r\u0026#34;exec-opts\u0026#34;: \\[\u0026#34;native.cgroupdriver=systemd\u0026#34;\\],\r\u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;,\r\u0026#34;log-opts\u0026#34;: {\r\u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;\r},\r\u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;\r}\rEOF Restart the docker service to make sure the new configuration is applied\nsudo systemctl daemon-reload \u0026amp;\u0026amp; sudo systemctl restart docker Installing kubeadm, kubelet and kubectl Update the apt package index and install packages needed to use the Kubernetes apt repository\nsudo apt-get update\rsudo apt-get install -y apt-transport-https ca-certificates curl Download the Google Cloud public signing key\nsudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg Add the Kubernetes apt repository\necho \u0026#34;deb \\[signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg\\] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list Update apt package index, install kubelet, kubeadm and kubectl\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y kubelet=1.23.1‚Äì00 kubectl=1.23.1‚Äì00 kubeadm=1.23.1‚Äì00 Initializing the control-plane node At this point, we have 3 nodes with docker, kubeadm , kubelet , and kubectl installed. Now we must initialize the Kubernetes master, which will manage the whole cluster and the pods running within the cluster kubeadm init by specifiy the address of the master node and the ipv4 address pool of the pods .\nsudo kubeadm init - apiserver-advertise-address=192.168.1.13 - pod-network-cidr=10.1.0.0/16 You should wait a few minutes until the initialization is completed. The first initialization will take a lot of time so be patient .\nConfiguring kubectl As known, the kubectl is a command line tool for performing actions on your cluster. So we must to configure kubectl . Run the following command from your master node :\nmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config Installing Calico CNI Calico provides network and network security solutions for containers. Calico is best known for its performance, flexibility and power. Usecases: Calico can be used within a lot of Kubernetes platforms (kops, Kubespray, docker enterprise, etc.) to block or allow traffic between pods, namespaces .\nInstall Tigera Calico operator\nkubectl create -f \u0026#34;https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml\u0026#34; The Tigera Operator is a Kubernetes operator which manages the lifecycle of a Calico or Calico Enterprise installation on Kubernetes. Its goal is to make installation, upgrades, and ongoing lifecycle management of Calico and Calico Enterprise as simple and reliable as possible.\nDownload the custom-resources.yaml manifest and change it . The Calico has a default pod‚Äôs CIDR value. But in our example, we set the ‚Äî pod-netwokr-cidr=10.1.0.0/16 . So we must change the valueof pod network CIDR in custom-resources.yaml\nwget \u0026#34;https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml\u0026#34; Now we edit this file before create the Calico pods\n\\# This section includes base Calico installation configuration.\r\\# For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation\rapiVersion: operator.tigera.io/v1\rkind: Installation\rmetadata:\rname: default\rspec:\r\\# Configures Calico networking.\rcalicoNetwork:\r\\# Note: The ipPools section cannot be modified post-install.\ripPools:\r\\- blockSize: 26\rcidr: 10.1.0.0/16\rencapsulation: VXLANCrossSubnet\rnatOutgoing: Enabled\rnodeSelector: all()\r\\- -\r\\# This section configures the Calico API server.\r\\# For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer\rapiVersion: operator.tigera.io/v1\rkind: APIServer\rmetadata:\rname: default\rspec: {} After Editing the custom-resources.yaml file. Run the following command\nkubectl create -f \u0026#34;custom-resources.yaml\u0026#34; Before you can use the cluster, you must wait for the pods required by Calico to be downloaded. You must wait until you find all the pods running and ready!\nkubectl get pods - all-namespaces Joining nodes To join the worker node to the master node , we need to run the following command\nsudo kubeadm token create - print-join-command Now we copy paste the command on the worker node\nsudo kubeadm join 192.168.1.13:6443 - token ekbcke.gqabooulf01ht13t - discovery-token-ca-cert-hash sha256:fbcded8c93105c67720c5e3d6c372427280b2f3805d71e88729cfed48fb18abc now we finally run\nkubectl get nodes and that‚Äôs what we get\nI was lazy to launch the second worker :D, i hope you enjoyed reading this article .\n","permalink":"https://malek-zaag.netlify.app/blog/kubernetes-on-local/","summary":"Kubernetes setup locally using kubeadm Hello reader, today i will show you in this article how i managed to install a Kubernetes cluster on my computer using kubeadm, this cluster contained two worker nodes with 2 GB of ram each alongside with one master node (2.5 GB of ram) so it was pretty heavy on my computer to be honest.\nBut first of all, let‚Äôs talk about kubernetes.\nKubernetes So, kubernetes(we will call it k8s) is a container orchestration tool which allows you to run multiple containerized services and it let‚Äôs you to scale your application with ease.","title":"I installed the kubernetes cluster locally"},{"content":"4 in a row The challenge was given as a 150x150 png file and as the description mentioned it was about png color channels and specifically the alpha value of a pixel.\nWe can read from the description that the flag was hidden in the alpha value of every pixel that its order is divisible by 4 . But before hacking into the challenge, let‚Äôs talk about the png color channel and how the data is embedded in each pixel.Each pixel in an image is represented by the model RGBA(red,green,blue,alpha) and these pixels are presenting the image as a matrix.\nSo the description says that the author is hiding the flag in the diagonal pixels and he gives the formula to extract it , so i concluded with a script doing it\nfrom PIL import Image image = Image.open(\u0026#34;00000000.png\u0026#34;).convert(\u0026#39;RGBA\u0026#39;) pixeldata = list(image.getdata()) flag=[] for i,pixel in enumerate(pixeldata): if i % 4 ==0: flag.append(255 - pixel[3]) res=[chr(flag[i]) for i in range(len(flag)) if flag[i]!= 0] print(res) joined= \u0026#34;\u0026#34;.join(res) print(joined) image.putdata(pixeldata) image.save(\u0026#34;output.png\u0026#34;) and the flag was Securinets{PiLl0W_Pyth0N_1S_Awe5oMe}\n5 in a row The second challengee was given as a 150x150 png file and as the description mentioned it was about png color channels and specifically the alpha value of a pixel.\nWe can read from the description that the flag was hidden in the alpha value of every pixel that its order is divisible by 5 and i == j (diagonal of the image) . But before we jump to the extracting part let‚Äôs talk about the png color channel and how the data is embedded in each pixel.Each pixel in an image is represented by the model RGBA(red,green,blue,alpha) and these pixels are presenting the image as a matrix.\nSo the description says that the author is hiding the flag in the diagonal pixels and he gives the formula to extract it , so i concluded with a script doing it\nfrom PIL import Image image = Image.open(\u0026#34;00000000.png\u0026#34;).convert(\u0026#39;RGBA\u0026#39;) pixeldata = list(image.getdata()) px = image.load() flag=\u0026#34;\u0026#34; for i in range(150): for j in range(150): if (i==j) and (i % 5 ==0): flag+=chr(255 - px[i,i][-1]^(px[i,i][0]^((px[i,i][1] + px[i,i][2])%3))) print(flag) image.save(\u0026#34;output.png\u0026#34;) and the flag was Securinets{AlpH4_1S_vEry_H4rD}\n","permalink":"https://malek-zaag.netlify.app/blog/securinets-friendly-ctf-2k22/","summary":"4 in a row The challenge was given as a 150x150 png file and as the description mentioned it was about png color channels and specifically the alpha value of a pixel.\nWe can read from the description that the flag was hidden in the alpha value of every pixel that its order is divisible by 4 . But before hacking into the challenge, let‚Äôs talk about the png color channel and how the data is embedded in each pixel.","title":"Securinets Friendly CTF 2K22 forensics writeup"},{"content":"This document describes how to install the NGINX Ingress Controller in your Kubernetes cluster using kubectl .\nWhat is Ingress ? Ingress is a native Kubernetes resource like pods, deployments, etc. Using ingress, you can maintain the DNS routing configurations. The ingress controller does the actual routing by reading the routing rules from ingress objects stored in etcd.\nBefore Ingress ? Before the Kubernetes Ingress was stable, a custom Nginx or an HAproxy kubernetes deployment would be exposed as a Loadbalancer service for routing external traffic to the internal cluster services.\nKubernetes Ingress : How it works? Actual Ingress relies on two concepts\nKubernetes Ingress Resource: Kubernetes ingress resource is responsible for storing DNS routing rules in the cluster.\nKubernetes Ingress Controller: Kubernetes ingress controllers (Nginx/HAProxy etc.) are responsible for routing by accessing the DNS rules applied through ingress resources.\nSetup First, we will understand all the associated Kubernetes objects by deploying Nginx controllers using YAML manifests. Once we have the understanding, we will deploy it using kubectl.\nInstall NGINX Ingress Controller An ingress controller, because it is a core component of Kubernetes, requires specific configuration to be performed at the cluster level as part of installation. The NGINX project simplifies this by providing a single deployment yaml file that captures all the required steps for the cluster configuration :\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml Output:\nExposing the NGINX Ingress Controller The above command will by default expose the NGINX Ingress Controller to the outside world to allow it to start receiving connections.\nValidate the NGINX Ingress Controller kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx\rkubectl get services ingress-nginx-controller --namespace=ingress-nginx Exposing Services using NGINX Ingress Controller Now we will be wirting the Ingress manifest :\napiVersion: networking.k8s.io/v1\rkind: Ingress\rmetadata:\rname: hello-world\rannotations:\rkubernetes.io/ingress.class: \u0026#34;nginx\u0026#34;\rnginx.ingress.kubernetes.io/rewrite-target: /\rspec:\rrules:\r- http:\rpaths:\r- pathType: Exact\rpath: /bookstore\rbackend:\rservice:\rname: app1-svc\rport:\rnumber: 80\r- pathType: Exact\rpath: /library\rbackend:\rservice:\rname: app2-svc\rport:\rnumber: 80 Testing out things :\nAfter deploying our applications manifests :\nWe can go the LoadBalancer IP and test things :\nand the bookstore :\nSo, to conclude, we demonstrated how to successfully deploy an Ingress controller in Azure Kubernetes Service and serve two ClusterIP services in the cluster .\n","permalink":"https://malek-zaag.netlify.app/blog/nginx-ingress-cloud/","summary":"This document describes how to install the NGINX Ingress Controller in your Kubernetes cluster using kubectl .\nWhat is Ingress ? Ingress is a native Kubernetes resource like pods, deployments, etc. Using ingress, you can maintain the DNS routing configurations. The ingress controller does the actual routing by reading the routing rules from ingress objects stored in etcd.\nBefore Ingress ? Before the Kubernetes Ingress was stable, a custom Nginx or an HAproxy kubernetes deployment would be exposed as a Loadbalancer service for routing external traffic to the internal cluster services.","title":"Setup nginx Ingress in cloud"},{"content":"1. Azure Fundamentals Date Earned: July 30, 2022 2. Azure Administrator Associate Date Earned: September 22, 2023 ","permalink":"https://malek-zaag.netlify.app/certifications/azure/","summary":"1. Azure Fundamentals Date Earned: July 30, 2022 2. Azure Administrator Associate Date Earned: September 22, 2023 ","title":"Microsoft Azure Certifications"},{"content":"Development of a web interface for managing and monitoring virtual machines in the Cloud In this article i m going to resume our end of year project that consists on the development of a web interface to create , manage and monitor virtual machines in the Cloud . The application will take CloudiVops as name and i will present for you the main features and the process of the development of this application\nTools to use ReactJS , NodeJS , MongoDB , ChartJS\nJenkins\nAnsible\nTerraform\nAzure\nGithub Repo Click Here\nHow CloudiVops works ? Once the user access to our application he will be face to a web interface after the creation of an account and the login the user will find a form where he can fulfill the desired infrastructure and the he can choose to configure the existent infra by our application that it will acts as an intermediary between the user and Azure because all the infra will be set up on Azure (me and my team we choosed azure so that we can use our subscription that contains 100$) our application will set up the infra and do all the configurations using pipelines and to understand more the workflow lets go through the next diagram\nworkflow of the Application Well this can make the system of cloudivops more clear\nOur system contains the frontend part developed on reactJS and the backend that is developed with expressJS and that will initiate the pipelines and will play a role on the automation of the processes .The api will contains the functions for the login and signup and the functions to trigger the pipelines\nfirst of all the user will create and account which will be saved on our database then he will be redirected to the home page inside the home page the user will find a button to create a virtual machine or a button to navigate to the dashboard and configure the VMs that exists once the form is submitted the function called is to trigger pipeline and that make a call to jenkins api and jenkins api will initiate terraform api and this will create the desired virtual machine by a simple trigger to azure api and azure will return the details to the user through our front pages\nif the user wants to configure a VM our api will trigger a second jenkins pipeline and this time jenkins api will call ansible api to configure the desired resource inside azure and then the state of the configuration will be passed to the user on the front pages .\nlet me now give some definitions of the used tools for the automation\nJenkins Jenkins is a Java-based program considered as an automation build tool that should be installed on a dedicated server , it is used to configure the tasks , install the tools used for the build and automatically trigger the workflow\njenkins offer different jobs the most used one and suitable for our usecase is the pipeline job\nTerraform Terraform is an infrastructure as code tool that lets you build, change, and version infrastructure safely and efficiently.\nTerraform automates and manages the infrastructure so it is a tool used for infrastructure provisioning\nTerraform take the desired state from the provided code and set up the infrastructed that will be stored in a state file\nAnsible Ansible is like terraform an automation tool that helps to automate the IT tasks , by just making a configuration file we can make tasks from a machine to another and use the same file in different environments\nansible architecture is based on modules where a module is a small program that is responsible of doing the task and each module is responsible of a specific task it is pushed to the server than removed after it is done\nto know on which server it is gonna be pushed ansible use the inventory list that will get the hosts names and all the data about the ansible client\nthe ansible playbook contains a group of modules where each task performs the action that we want to do .the ansible playbook contains also the host to describe where the tasks are going to be running and also contains the remote user to tell with which user the tasks should be executed\nDevelopment of CloudiVops The home page will looks like this\nit is just a simple page that contains a description of our services , button for login and button for sign up\nonce logged in the email will appear in the place of the buttons of login and sign up and the page will looks like this\nIf the user wants to create a vm the vm creation will happens through this form\nand once the vm is created the dashboard will look like this\nSo here the user will find the IP of the machine the region and 2 buttons either to install docker or mysql and he can also delete the resource\nalso the dashboard contains monitoring graphs\nthe submit of the form will initiate jenkins pipeline and the page that we got on the front will gives an idea about the state of the build of the pipeline\nmeans that our infrastructure is gonna be created and once the pipeline is build succesfully we got a changed status\nand the same for the ansible pipeline\nThe code of the pipelines , the playbooks , the terraform templates are in the github repository\nSo that was a brieve summary of this web application that we would like to improve it in the future by adding more features\nFinally i hope that this article was usefull and it was not that much long\nthe full web application is in this repo where you can check also the full configuration files since the screenshots were so limited\nWas this helpful? Confusing? If you have any questions, feel free to comment below! Make sure to follow on Linkedin and github if you‚Äôre interested in similar content and want to keep learning alongside me!\n","permalink":"https://malek-zaag.netlify.app/projects/cloudivops/","summary":"Development of a web interface for managing and monitoring virtual machines in the Cloud In this article i m going to resume our end of year project that consists on the development of a web interface to create , manage and monitor virtual machines in the Cloud . The application will take CloudiVops as name and i will present for you the main features and the process of the development of this application","title":"VM creation SaaS"},{"content":"Description Guide: Yassine Mezrani, Moez Mhiri Migrated two microservices applications (ACM and DFM) that were running on Docker Compose to a local deployed Kubernetes cluster Worked with Dev Team to understand the architecture of the 11-microservices application (ACM) Showcased the benefits of migrating the local Kubernetes cluster to the cloud infrastructure through meetups with the Tech Lead and the scrum master Skills: Kubernetes ¬∑ RedHat CentOS ¬∑ Microsoft Azure ¬∑ Spring Boot ","permalink":"https://malek-zaag.netlify.app/experience/talys/","summary":"Description Guide: Yassine Mezrani, Moez Mhiri Migrated two microservices applications (ACM and DFM) that were running on Docker Compose to a local deployed Kubernetes cluster Worked with Dev Team to understand the architecture of the 11-microservices application (ACM) Showcased the benefits of migrating the local Kubernetes cluster to the cloud infrastructure through meetups with the Tech Lead and the scrum master Skills: Kubernetes ¬∑ RedHat CentOS ¬∑ Microsoft Azure ¬∑ Spring Boot ","title":"DevOps Engineer Intern"},{"content":"Apache Kafka Introduction In this article, we are going to talk about apache kafka which is an open-source distributed streaming platform that was developed by the Apache Software Foundation. It is designed to handle large volumes of real-time data and enables the efficient processing, storage, and analysis of data streams.\nkafka Cluster cluster : Our cluster can be illustrated as below :\nProducer : In Kafka, a producer is a client application that is responsible for publishing data to one or more Kafka topics. Producers can be written in various programming languages such as Java, Python, or Scala using the Kafka client libraries.\nConsumer : In Kafka, a consumer is a client application that is responsible for reading data from one or more Kafka topics. Consumers can be written in various programming languages such as Java, Python, or Scala using the Kafka client libraries. Consumers can also be organized into consumer groups, where each group consists of one or more consumers that share the workload of consuming messages from a set of partitions. Kafka ensures that each partition is only consumed by one consumer within a group, which allows for load balancing and fault-tolerance.\nBroker : In Kafka, a broker is a server that is responsible for storing and managing Kafka topics and partitions. Brokers receive messages from producers and serve them to consumers, and they also replicate messages across a cluster of brokers for fault-tolerance and scalability. Topic is a category or feed name to which producers can write messages and consumers can subscribe to read messages. A topic is divided into one or more partitions, where each partition is an ordered, immutable sequence of messages.\nReplication factor : The replication factor in apache Kafka is the number of replicas (copies) of a partition that are maintained across multiple brokers in a cluster. A higher replication factor provides better fault tolerance and durability, as it ensures that the data is still available even if one or more brokers or disks fail.\nDemo For the sake of this demo, we are going to use some ready docker compose file to summon up apache kafka cluster with 1 broker and with apache zookeeper.\nversion: \u0026#34;3\u0026#34; services: zookeeper: image: confluentinc/cp-zookeeper:7.3.2 container_name: zookeeper environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 broker: image: confluentinc/cp-kafka:7.3.2 container_name: broker ports: - \u0026#34;9092:9092\u0026#34; depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: \u0026#34;zookeeper:2181\u0026#34; KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 This file can be found at confluent.\napache zookeeper : Apache ZooKeeper is a distributed coordination service that is often used with Apache Kafka to manage and coordinate a cluster of brokers. ZooKeeper provides a highly available and fault-tolerant way of storing and managing configuration information, naming, synchronization, and other shared data among the nodes in a distributed system. So, we start by executing the following command the summon the broker and zookeeper containers :\nbash docker compose up -d\rOur containers are ready : Now we write our producer and consumer codes :\nimport data import kafka import json import time def data_to_json(): return json.dumps(data.data_faker()).encode(\u0026#34;utf-8\u0026#34;) producer=kafka.KafkaProducer(bootstrap_servers=[\u0026#39;localhost:9092\u0026#39;]) def produce(): while 1==1: print(\u0026#34;here\u0026#34;) producer.send(\u0026#34;registred_user\u0026#34;,json.dumps(data.data_faker()).encode()) time.sleep(3) produce() import kafka consumer=kafka.KafkaConsumer(\u0026#39;registred_user\u0026#39;, group_id=\u0026#39;my_favorite_group\u0026#39;,bootstrap_servers=[\u0026#34;localhost:9092\u0026#34;]) def consume(): for msg in consumer: print(msg.value) consume() Now we run our scripts, producer on the left and consumer on the right : and voila, every message generated by the producer is read by the consumer undet same topic.\nWas this helpful? Confusing? If you have any questions, feel free to comment below! Make sure to follow on Linkedin : https://www.linkedin.com/in/malekzaag/ and github: https://github.com/Malek-Zaag if you‚Äôre interested in similar content and want to keep learning alongside me!\n","permalink":"https://malek-zaag.netlify.app/blog/apache-kafka-demo/","summary":"Apache Kafka Introduction In this article, we are going to talk about apache kafka which is an open-source distributed streaming platform that was developed by the Apache Software Foundation. It is designed to handle large volumes of real-time data and enables the efficient processing, storage, and analysis of data streams.\nkafka Cluster cluster : Our cluster can be illustrated as below :\nProducer : In Kafka, a producer is a client application that is responsible for publishing data to one or more Kafka topics.","title":"Apache Kafka"},{"content":"E-commerce Website : Overview : This project is among my first web projects designed in my early steps of self learning. It is an e-commerce shop to sell musical instruments. You need to create an account in order to view cart, view shop and make orders, it includes also a payment module for users.\nGitHub Repo\n","permalink":"https://malek-zaag.netlify.app/projects/e-commerce-shop/","summary":"E-commerce Website : Overview : This project is among my first web projects designed in my early steps of self learning. It is an e-commerce shop to sell musical instruments. You need to create an account in order to view cart, view shop and make orders, it includes also a payment module for users.\nGitHub Repo","title":"E-commerce Website"},{"content":"Gnutella Project Overview : This project was issued to me during my academic studies. It consists of creating a simple application that uses the peer-to-peer protocol Gnutella.\nGnutella : The Gnutella protocol is a peer-to-peer (P2P) file-sharing protocol that allows users to share files directly with each other over the Internet without the need for a central server. It was developed in 2000 and is based on a decentralized network architecture that enables users to search for and download files from other users\u0026rsquo; computers. Implementation : For implementing the application, i used tkinter for the graphical interface and sockets for communication between different clients, you can find all the code in the github repo down below. GithHub Repo\n","permalink":"https://malek-zaag.netlify.app/projects/gnutella-project/","summary":"Gnutella Project Overview : This project was issued to me during my academic studies. It consists of creating a simple application that uses the peer-to-peer protocol Gnutella.\nGnutella : The Gnutella protocol is a peer-to-peer (P2P) file-sharing protocol that allows users to share files directly with each other over the Internet without the need for a central server. It was developed in 2000 and is based on a decentralized network architecture that enables users to search for and download files from other users\u0026rsquo; computers.","title":"Python implementation for the Gnutella Protocol"},{"content":"Todo List This app is built with NextJS and ExpressJS as a backend library. I used MongoDB as a database for my users and their todos. This app allows users to authenticate, register and create their todos that will be saved to the database.\nArchitecture UML diagrams This is the Usecase Diagram of the app :\nand this is the entity-relationship Diagram :\nDocker Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker‚Äôs methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\nhttps://www.docker.com/\nInstallation Clone repo into your directory , you need docker to run it .\ncd my-project docker-compose -f Docker-compose.yaml.up The application runs now at http://localhost:3000\nGithHub Repo\n","permalink":"https://malek-zaag.netlify.app/projects/todo-app-ts/","summary":"Todo List This app is built with NextJS and ExpressJS as a backend library. I used MongoDB as a database for my users and their todos. This app allows users to authenticate, register and create their todos that will be saved to the database.\nArchitecture UML diagrams This is the Usecase Diagram of the app :\nand this is the entity-relationship Diagram :\nDocker Docker is an open platform for developing, shipping, and running applications.","title":"Todo list using ReactJS and NodeJS"},{"content":"Description Guide: Henda Regaya Development of web based OCR application to check frauds and errors in banking checks. Skills: Google Cloud Platform (GCP) ¬∑ React.js ¬∑ Java ¬∑ Spring Boot\n","permalink":"https://malek-zaag.netlify.app/experience/true-delta/","summary":"Description Guide: Henda Regaya Development of web based OCR application to check frauds and errors in banking checks. Skills: Google Cloud Platform (GCP) ¬∑ React.js ¬∑ Java ¬∑ Spring Boot","title":"Remote Web Developer Intern"},{"content":"Description Guide: Neirouz Jbira Development of fullstack web application for teleconsultation.Development of fullstack web application for teleconsultation. Skills: Express.js ¬∑ React.js ¬∑ MongoDB ¬∑ Node.js\n","permalink":"https://malek-zaag.netlify.app/experience/dwaya/","summary":"Description Guide: Neirouz Jbira Development of fullstack web application for teleconsultation.Development of fullstack web application for teleconsultation. Skills: Express.js ¬∑ React.js ¬∑ MongoDB ¬∑ Node.js","title":"Web Developer Intern"}]